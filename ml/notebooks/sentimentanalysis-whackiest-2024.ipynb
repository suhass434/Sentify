{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers praw vaderSentiment textblob genai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:59:23.249816Z","iopub.execute_input":"2024-12-22T04:59:23.250238Z","iopub.status.idle":"2024-12-22T04:59:31.556377Z","shell.execute_reply.started":"2024-12-22T04:59:23.250205Z","shell.execute_reply":"2024-12-22T04:59:31.554863Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\nRequirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\nRequirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\nCollecting genai\n  Downloading genai-2.1.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\nRequirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\nRequirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\nRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.2.4)\nCollecting ipython<9.0.0,>=8.10.0 (from genai)\n  Downloading ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting openai<0.28.0,>=0.27.0 (from genai)\n  Downloading openai-0.27.10-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from genai) (0.9.0)\nCollecting tiktoken<0.4.0,>=0.3.2 (from genai)\n  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (4.4.2)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (1.2.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (0.19.2)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (4.9.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython<9.0.0,>=8.10.0->genai) (2.18.0)\nCollecting stack_data (from ipython<9.0.0,>=8.10.0->genai)\n  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting traitlets>=5.13.0 (from ipython<9.0.0,>=8.10.0->genai)\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.16.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.0->genai) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<9.0.0,>=8.10.0->genai) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<9.0.0,>=8.10.0->genai) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9.0.0,>=8.10.0->genai) (0.2.13)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->genai) (4.0.3)\nCollecting executing>=1.2.0 (from stack_data->ipython<9.0.0,>=8.10.0->genai)\n  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython<9.0.0,>=8.10.0->genai) (3.0.0)\nCollecting pure-eval (from stack_data->ipython<9.0.0,>=8.10.0->genai)\n  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nDownloading genai-2.1.0-py3-none-any.whl (16 kB)\nDownloading ipython-8.31.0-py3-none-any.whl (821 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openai-0.27.10-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\nDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\nDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nInstalling collected packages: pure-eval, traitlets, executing, tiktoken, stack_data, ipython, openai, genai\n  Attempting uninstall: traitlets\n    Found existing installation: traitlets 5.7.1\n    Uninstalling traitlets-5.7.1:\n      Successfully uninstalled traitlets-5.7.1\n  Attempting uninstall: tiktoken\n    Found existing installation: tiktoken 0.8.0\n    Uninstalling tiktoken-0.8.0:\n      Successfully uninstalled tiktoken-0.8.0\n  Attempting uninstall: ipython\n    Found existing installation: ipython 7.34.0\n    Uninstalling ipython-7.34.0:\n      Successfully uninstalled ipython-7.34.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.31.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed executing-2.1.0 genai-2.1.0 ipython-8.31.0 openai-0.27.10 pure-eval-0.2.3 stack_data-0.6.3 tiktoken-0.3.3 traitlets-5.14.3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:45:10.998743Z","iopub.execute_input":"2024-12-22T04:45:10.999061Z","iopub.status.idle":"2024-12-22T04:45:24.120552Z","shell.execute_reply.started":"2024-12-22T04:45:10.999034Z","shell.execute_reply":"2024-12-22T04:45:24.119202Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nfrom transformers import pipeline\nimport praw\nimport requests\nimport spacy\nfrom datetime import datetime, timedelta\nfrom collections import Counter\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.spatial.distance import cosine\nimport google.generativeai as genai\n\nfrom kaggle_secrets import UserSecretsClient\nimport os\nsecrets = UserSecretsClient()\n\nclass EnhancedContentAnalyzer:\n    def __init__(self, reddit_credentials, news_api_key, gemini_api_key):\n        self.vader = SentimentIntensityAnalyzer()\n        self.reddit = praw.Reddit(**reddit_credentials)\n        self.news_api_key = news_api_key\n        self.nlp = spacy.load('en_core_web_sm')\n        \n        # Initialize models\n        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n        self.emotion_classifier = pipeline(\"text-classification\", \n                                        model=\"j-hartmann/emotion-english-distilroberta-base\")\n        \n        # Sentiment thresholds\n        self.sentiment_labels = {\n            range(0, 35): \"Negative\",\n            range(35, 65): \"Neutral\",\n            range(65, 101): \"Positive\"\n        }\n        genai.configure(api_key = gemini_api_key)\n        self.gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    def clean_text(self, text):\n        \"\"\"Enhanced text cleaning with entity and key term retention\"\"\"\n        if not isinstance(text, str):\n            return \"\"\n        \n        # Basic cleaning\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'@\\w+', '', text)\n        text = re.sub(r'#\\w+', '', text)\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        \n        # SpaCy processing\n        doc = self.nlp(text)\n        \n        # Keep named entities, important parts of speech, and descriptive terms\n        important_tokens = []\n        for token in doc:\n            if (token.ent_type_ or  # Named entities\n                token.pos_ in ['ADJ', 'VERB', 'NOUN', 'ADV'] or  # Important POS\n                not token.is_stop):  # Non-stop words\n                important_tokens.append(token.text.lower())\n        \n        return ' '.join(important_tokens)\n\n    def get_combined_sentiment(self, text):\n        \"\"\"Enhanced sentiment analysis with normalized scoring and labeling\"\"\"\n        cleaned_text = self.clean_text(text)\n        \n        # VADER sentiment\n        vader_scores = self.vader.polarity_scores(cleaned_text)\n        vader_compound = vader_scores['compound']\n        \n        # TextBlob sentiment\n        textblob_score = TextBlob(cleaned_text).sentiment.polarity\n        \n        # Define weights for ensemble\n        weights = {\n            'vader': 0.6,\n            'textblob': 0.4\n        }\n        \n        # Calculate weighted ensemble score\n        ensemble_score = (\n            weights['vader'] * vader_compound +\n            weights['textblob'] * textblob_score\n        )\n        \n        # Normalize to 0-100 scale\n        normalized_score = int((ensemble_score + 1) * 50)\n        \n        # Get sentiment label\n        sentiment_label = next(\n            (label for range_obj, label in self.sentiment_labels.items() \n             if normalized_score in range_obj),\n            \"Neutral\"\n        )\n        \n        return {\n            'score': normalized_score,\n            'label': sentiment_label,\n            'raw_scores': {\n                'vader': vader_compound,\n                'textblob': textblob_score\n            }\n        }\n\n    def get_aspect_based_sentiment(self, text, aspects):\n        \"\"\"Enhanced aspect-based sentiment analysis\"\"\"\n        doc = self.nlp(text)\n        aspect_sentiments = {}\n        \n        for aspect in aspects:\n            relevant_sentences = []\n            for sent in doc.sents:\n                # Check for aspect and its synonyms\n                sent_lower = sent.text.lower()\n                if aspect.lower() in sent_lower:\n                    relevant_sentences.append(sent.text)\n            \n            if relevant_sentences:\n                # Calculate sentiment for each relevant sentence\n                sentiments = [self.get_combined_sentiment(sent)['score'] \n                            for sent in relevant_sentences]\n                \n                aspect_sentiments[aspect] = {\n                    'score': np.mean(sentiments),\n                    'count': len(relevant_sentences),\n                    'sample_text': relevant_sentences[0] if relevant_sentences else None\n                }\n        \n        return aspect_sentiments\n\n    def analyze_emotions(self, text):\n        \"\"\"Enhanced emotion detection\"\"\"\n        try:\n            emotion_result = self.emotion_classifier(text)[0]\n            return {\n                'emotion': emotion_result['label'],\n                'confidence': emotion_result['score']\n            }\n        except:\n            return {'emotion': 'neutral', 'confidence': 1.0}\n\n    def deduplicate_content(self, texts, threshold=0.8):\n        \"\"\"Remove near-duplicate content using TF-IDF and cosine similarity\"\"\"\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform(texts)\n        \n        unique_indices = []\n        for i in range(len(texts)):\n            is_unique = True\n            for j in range(i):\n                if j in unique_indices:\n                    similarity = 1 - cosine(\n                        tfidf_matrix[i].toarray().flatten(),\n                        tfidf_matrix[j].toarray().flatten()\n                    )\n                    if similarity > threshold:\n                        is_unique = False\n                        break\n            if is_unique:\n                unique_indices.append(i)\n        \n        return unique_indices\n        \n    def fetch_news(self, query, days=7):\n            \"\"\"Fetch news articles from NewsAPI\"\"\"\n            url = 'https://newsapi.org/v2/everything'\n            date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n            \n            params = {\n                'q': query,\n                'from': date_from,\n                'sortBy': 'relevancy',\n                'apiKey': self.news_api_key,\n                'language': 'en'\n            }\n            \n            try:\n                response = requests.get(url, params=params)\n                articles = response.json().get('articles', [])\n                return [{\n                    'source': 'news',\n                    'title': article['title'],\n                    'text': f\"{article['title']} {article['description']}\",\n                    'url': article['url'],\n                    'published_at': article['publishedAt']\n                } for article in articles[:10]]  # Top 10 most relevant articles\n            except:\n                return []\n\n    def fetch_reddit_posts(self, query, limit=10):\n        \"\"\"Fetch Reddit posts\"\"\"\n        posts = []\n        subreddit = self.reddit.subreddit('all')\n        \n        for post in subreddit.search(query, sort='relevance', limit=limit):\n            posts.append({\n                'source': 'reddit',\n                'title': post.title,\n                'text': f\"{post.title} {post.selftext}\",\n                'url': f\"https://reddit.com{post.permalink}\",\n                'score': post.score\n            })\n            \n        return posts\n        \n    def fetch_content(self, query, limit=50):\n        \"\"\"Fetch and aggregate content from multiple sources\"\"\"\n        # Fetch news articles\n        news_articles = self.fetch_news(query)\n        \n        # Fetch Reddit posts\n        reddit_posts = self.fetch_reddit_posts(query, limit=limit)\n        \n        # Combine all content\n        all_content = news_articles + reddit_posts\n        \n        # Deduplicate content\n        unique_texts = [item['text'] for item in all_content]\n        unique_indices = self.deduplicate_content(unique_texts)\n        \n        return [all_content[i] for i in unique_indices]\n\n    def predict_trend(self, sentiments, window_size=7):\n        \"\"\"Enhanced trend prediction with confidence scoring\"\"\"\n        if len(sentiments) < window_size:\n            return {\n                'trend': \"Insufficient data\",\n                'confidence': 0.0\n            }\n        \n        sentiment_series = pd.Series(sentiments)\n        rolling_mean = sentiment_series.rolling(window=window_size).mean()\n        \n        current_trend = rolling_mean.iloc[-1] - rolling_mean.iloc[-2]\n        confidence = min(abs(current_trend) * 10, 1.0)\n        \n        if current_trend > 0.05:\n            trend = \"Upward\"\n        elif current_trend < -0.05:\n            trend = \"Downward\"\n        else:\n            trend = \"Stable\"\n            \n        return {\n            'trend': trend,\n            'confidence': confidence\n        }\n\n    def generate_summary(self, content_items):\n        \"\"\"Generate comprehensive summary using all content\"\"\"\n        # Combine all text\n        all_text = \" \".join([item['text'] for item in content_items])\n        \n        # Truncate the text if it exceeds the maximum token limit for the model\n        max_tokens = 512\n        truncated_text = all_text[:max_tokens]\n        text_was_truncated = len(all_text) > max_tokens\n        \n        try:\n            # Generate summary using Google Gemini\n            prompt = (\n                \"Please summarize the following content in 3-5 lines, focusing on the key points. \"\n                \"Ensure the summary is concise and covers the main aspects of the text.\"\n                f\"\\n\\n{truncated_text}\"\n            )\n            response = self.gemini_model.generate_content(prompt)\n            initial_summary = response.text.strip()\n\n            # Analyze emotions in the content\n            emotions = [self.analyze_emotions(item['text']) for item in content_items]\n            dominant_emotion = Counter(\n                [e['emotion'] for e in emotions]\n            ).most_common(1)[0][0]\n            \n            # Calculate overall sentiment\n            sentiments = [self.get_combined_sentiment(item['text'])['score'] \n                        for item in content_items]\n            avg_sentiment = np.mean(sentiments)\n            if avg_sentiment < 35:\n                sentiment_label = \"Negative\"\n            elif avg_sentiment < 65:\n                sentiment_label = \"Neutral\"\n            else:\n                sentiment_label = \"Positive\"   \n                \n            # Get trend\n            trend_info = self.predict_trend(sentiments)\n    \n            # Enhance summary with additional insights\n            enhanced_summary = (\n                f\"{initial_summary}\\n\\n\"\n                f\"Overall Sentiment: {sentiment_label} \"\n                f\"({avg_sentiment:.1f}/100)\\n\"\n                f\"Dominant Emotion: {dominant_emotion.title()}\\n\"\n                f\"Trend: {trend_info['trend']} (Confidence: {trend_info['confidence']:.2f})\"\n            )\n            \n            # # Append \"....\" if text was truncated\n            # if text_was_truncated:\n            #     enhanced_summary += \" ....\"\n            \n            return enhanced_summary\n        \n        except Exception as e:\n            return f\"Error generating summary: {str(e)}\"\n\n    def analyze_query(self, query, aspects=None):\n        \"\"\"Main analysis method\"\"\"\n        if aspects is None:\n            aspects = [\"price\", \"quality\", \"features\", \"service\"]\n        \n        # Fetch and analyze content\n        content_items = self.fetch_content(query)\n        \n        # Analyze each piece of content\n        analyzed_content = []\n        for item in content_items:\n            sentiment = self.get_combined_sentiment(item['text'])\n            emotions = self.analyze_emotions(item['text'])\n            aspect_sentiments = self.get_aspect_based_sentiment(item['text'], aspects)\n            \n            analyzed_content.append({\n                'source': item['source'],\n                'title': item.get('title', ''),\n                'url': item.get('url', ''),\n                'sentiment': sentiment,\n                'emotions': emotions,\n                'aspect_sentiments': aspect_sentiments\n            })\n        \n        # Generate overall summary\n        summary = self.generate_summary(content_items)\n        \n        # Calculate aggregated metrics\n        sentiments = [item['sentiment']['score'] for item in analyzed_content]\n        trend = self.predict_trend(sentiments)\n        \n        return {\n            'summary': summary,\n            'analyzed_content': analyzed_content,\n            'trend': trend,\n            'aspects': {\n                aspect: {\n                    'avg_score': np.mean([\n                        content['aspect_sentiments'].get(aspect, {}).get('score', 0)\n                        for content in analyzed_content\n                        if aspect in content['aspect_sentiments']\n                    ])\n                }\n                for aspect in aspects\n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T05:08:44.805413Z","iopub.execute_input":"2024-12-22T05:08:44.805801Z","iopub.status.idle":"2024-12-22T05:08:44.840909Z","shell.execute_reply.started":"2024-12-22T05:08:44.805770Z","shell.execute_reply":"2024-12-22T05:08:44.839483Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def main(query):\n    # Initialize with credentials\n    reddit_credentials = {\n        'client_id': secrets.get_secret(\"REDDIT_CLIENT_ID\"),\n        'client_secret': secrets.get_secret(\"REDDIT_CLIENT_SECRET\"),\n        'user_agent': secrets.get_secret(\"REDDIT_USER_AGENT\")\n    }\n    news_api_key = secrets.get_secret(\"NEWS_API_KEY\")\n    gemini_api_key = secrets.get_secret(\"GEMINI_API_KEY\")\n    analyzer = EnhancedContentAnalyzer(reddit_credentials, news_api_key, gemini_api_key)\n    \n    # Example analysis\n    aspects = [\"price\", \"features\", \"reliability\", \"support\"]\n    \n    results = analyzer.analyze_query(query, aspects)\n    \n    # Print results\n    print(\"\\nSummary:\")\n    print(results['summary'])\n    \n    print(\"\\nAspect Sentiments:\")\n    for aspect, data in results['aspects'].items():\n        print(f\"{aspect}: {data['avg_score']:.1f}/100\")\n    \n    print(\"\\nTop Content by Sentiment Impact:\")\n    sorted_content = sorted(\n        results['analyzed_content'],\n        key=lambda x: abs(x['sentiment']['score'] - 50),\n        reverse=True\n    )[:5]\n    \n    for content in sorted_content:\n        print(f\"\\nSource: {content['source']}\")\n        print(f\"Title: {content['title']}\")\n        print(f\"Sentiment: {content['sentiment']['label']} ({content['sentiment']['score']}/100)\")\n        print(f\"Dominant Emotion: {content['emotions']['emotion']}\")\n        print(f\"URL: {content['url']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T05:08:46.495063Z","iopub.execute_input":"2024-12-22T05:08:46.495429Z","iopub.status.idle":"2024-12-22T05:08:46.503540Z","shell.execute_reply.started":"2024-12-22T05:08:46.495401Z","shell.execute_reply":"2024-12-22T05:08:46.502166Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"main(\"Iphone 15\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T05:08:49.232780Z","iopub.execute_input":"2024-12-22T05:08:49.233249Z","iopub.status.idle":"2024-12-22T05:09:09.344340Z","shell.execute_reply.started":"2024-12-22T05:08:49.233208Z","shell.execute_reply":"2024-12-22T05:09:09.343140Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2596 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"\nSummary:\nDespite rumors, the iPhone 17 Pro will likely retain its triangular camera design.  A leaker refutes claims of a horizontal camera arrangement.  This contradicts recent reports suggesting a design change.  Furthermore, the AirTag 2 is anticipated (though details omitted).  Public interest in Apple's intelligence gathering capabilities related to iPhone purchases is deemed low.\n\nOverall Sentiment: Neutral (54.3/100)\nDominant Emotion: Neutral\nTrend: Upward (Confidence: 1.00)\n\nAspect Sentiments:\nprice: 64.2/100\nfeatures: 60.5/100\nreliability: nan/100\nsupport: 68.5/100\n\nTop Content by Sentiment Impact:\n\nSource: reddit\nTitle: My iPhone 15 review\nSentiment: Positive (84/100)\nDominant Emotion: neutral\nURL: https://reddit.com/r/iphone15/comments/1e5rco8/my_iphone_15_review/\n\nSource: reddit\nTitle: Just got my iphone 15 after 15 years of android\nSentiment: Positive (84/100)\nDominant Emotion: sadness\nURL: https://reddit.com/r/iphone15/comments/1hbq4ui/just_got_my_iphone_15_after_15_years_of_android/\n\nSource: reddit\nTitle: Thoughts on Iphone 13 and 15 being discontinued?\nSentiment: Negative (18/100)\nDominant Emotion: surprise\nURL: https://reddit.com/r/iphone/comments/1fewac4/thoughts_on_iphone_13_and_15_being_discontinued/\n\nSource: reddit\nTitle: I found an iPhone 15 max\nSentiment: Positive (82/100)\nDominant Emotion: surprise\nURL: https://reddit.com/r/iphone/comments/1b14mrn/i_found_an_iphone_15_max/\n\nSource: reddit\nTitle: iPhone 15 Alarms Failing To Go Off - Is This Happening To Anyone Else?\nSentiment: Negative (19/100)\nDominant Emotion: neutral\nURL: https://reddit.com/r/iphone/comments/18lwiz0/iphone_15_alarms_failing_to_go_off_is_this/\n","output_type":"stream"}],"execution_count":45}]}